using ReverseDiff
using Statistics

logsoftmax(y) = y.-log.(sum(exp.(y), dims=1))
logitcrossentropy(ŷ, y) = mean(.-sum(y .* logsoftmax(ŷ); dims=1))
crossentropy(ŷ, y) = -sum(y .* log.(ŷ))

lossfun = crossentropy

x = [1.0; 2.0; 3.0; 4.0; 5.0]

W1 = [
    0.5731809195726594 0.6648371921346286 1.2993364821337263 1.7673004203595648 2.3973935493382115;
    1.3454095474399999 0.7717639140663247 0.4680042029118444 0.9759970753615571 0.12931582541678244;
    1.335362103686751 0.9609312644677062 1.3767399069930406 0.629081172867276 0.45009484124196886;
    0.6335767397124203 0.43450802801331634 0.2674203454904461 0.5731809195726594 0.6648371921346286
]

W2 = [
    0.5731809195726594 0.6648371921346286 1.2993364821337263 1.7673004203595648;
    1.7673004203595648 2.3973935493382115 0.9201088691339511 1.3454095474399999;
    1.3454095474399999 0.7717639140663247 0.4680042029118444 0.9759970753615571
]

W3 = [
    0.5731809195726594 0.6648371921346286 1.2993364821337263;
    1.7673004203595648 2.3973935493382115 0.9201088691339511
]

dense1W(W) = tanh.(W*x)
dense2W(W) = tanh.(W2*dense1W(W))
dense3W(W) = tanh.(W3*dense2W(W))

dense1b(b) = tanh.(W1*x .+ b)
dense2b(b) = tanh.(W2*dense1b(b))

net(W) = lossfun(dense3W(W), ones(2))
ReverseDiff.gradient(net, W)