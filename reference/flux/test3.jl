using ReverseDiff
using Statistics

logsoftmax(y) = y.-log.(sum(exp.(y), dims=1))
logitcrossentropy(ŷ, y) = mean(.-sum(y .* logsoftmax(ŷ); dims=1))
crossentropy(ŷ, y) = -sum(y .* log.(ŷ))

lossfun = crossentropy

x = [1.0; 2.0; 3.0]

W1 = [
    0.5731809195726594 0.6648371921346286 1.2993364821337263;
    1.7673004203595648 2.3973935493382115 0.9201088691339511;
    1.3454095474399999 0.7717639140663247 0.4680042029118444;
    0.9759970753615571 0.12931582541678244 2.0914188568559973
]

W2 = [
    1.335362103686751 0.9609312644677062 1.3767399069930406 0.629081172867276;
    0.45009484124196886 0.6335767397124203 0.43450802801331634 0.2674203454904461
]

dense1(W) = tanh.(W*x)
dense2(W) = tanh.(W2*dense1(W))

net(W) = lossfun(dense2(W), ones(2))
ReverseDiff.gradient(net, W)